% !TEX root = thesis.tex
\documentclass[thesis.tex]{subfiles} 
\begin{document}

\chapter{Revised Requirements}
\label{chap:revised}
In this chapter we will analyze which parts of our exploratory prototype we can
utilize in our implementation. To that end we will examine every tool used and
arrive at a subset of these tools, which we will complement with a fresh set of
parts. Our goal is to have a plan laid out for the final architecture at the end
of this chapter.

The prototype featured a large amount of moving parts that were constructed for
the occasion. Among others this includes our templates, that were not mustache
templates but simple repeatable patterns in PHP. The recursive tree parser,
building a set of models and views for us to use on the client side is another
example of an ad-hoc constructed tool.

These parts bring with them their own set of problems and bugs.
Since they are custom developed in a limited time frame they will have coding
errors other seasoned related tools do not have.
We decided to develop these tools for our prototype regardless,
because evaluating alternatives that would fit the purpose precisely
would have taken up more time.
This is however not a sound strategy going forward, assuming we have the goal to
develop a reliable tool a developer may use to build web applications.
Succinctly put: There is no reason to reinvent the wheel.
Most of these parts have nothing to do directly with the concept of this thesis.
They are rather tools that help achieve the goals of it.
For a proper tool, that we can consider usable, to emerge from our process,
we will need to reduce the amount of said custom parts.
To that end we will first have to identify the superfluous parts of the
prototype, that can be replaced by existing well maintained tools. Once we have
achieved this, we can begin concentrating on the core of our concept and define
it with greater precision.

\section{Revised Goal}
\label{sec:revised-goal}
We set out with the goal to couple server-side templates with client side
models. In reality this goal consists of two parts:\
First we need to extract the dataset passed into the template engine together
with the template from the render template\footnote{Read: from the HTML in the browser.}.
In the prototype we did this by utilizing hard-coded XPaths. The motivation
behind hard-coding this part was the assumption that we would generate the those
paths dynamically in our real implementation.\
Once the dataset is rebuilt the second part consists of mapping its values
to models on the client side. In the prototype we realized the mapping process
by hard-coding the types of backbone fields into the ModelView.

Our final implementation will only feature the first of these processes.
This drastic change of our goal is grounded in the desire to create a tool that
is applicable in as many types of web applications as possible.
The mustache template engine is the only precondition for using the first part
of our mapping process. The second part however can have preconditions
other than backbone that were not explored in our prototype:\
To generate a mapping for client-side models, our tool will require to know the
origin of the values in the retrieved dataset\footnote{
	The dataset itself will contain only string values retrieved from the rendered
	template. Its structure can similarly only resemble the structure of the
	template.
}. These values usually originate in models on the server-side\footnote{
	They may also come from client-side models, where the mapping would make less
	sense, since the values are already mapped to the models.
	However chapter \ref{chap:eval} details why such a mapping would be
	advantageous regardless of that fact.
	\todo{Specific section}
}, where the developer may have chosen from any number of server-side languages
and model frameworks. Unless we require the developer to specify the types and
relations between models in a format our tool can understand, we will have to
choose a language and model framework to automate this process. This choice
reduces the applicability of our tool greatly. Leaving only the option of
requiring the developer to specify the relations. For big web applications this
requirement may slow the development process significantly.
For this reason we choose to pursue only the first part of the mapping process.
With that choice the implementation will no longer concentrate on mapping values
sent by the server via rendered templates to client-side models,
but on extracting data from rendered templates.

By concentrating on one part of the process we are also able to create a tool in
the proper sense. As a guide for the properties of such a tool we can apply
some of the rules set by Eric Steven Raymond in his book
``The Art of Unix Programming''.
\begin{citequote}{\cite[Chapter 1]{UXART}}
\begin{itemize}
	\item Rule of Modularity: Write simple parts connected by clean interfaces.
	\item Rule of Clarity: Clarity is better than cleverness.
	\item Rule of Composition: Design programs to be connected to other programs.
	\item Rule of Separation: Separate policy from mechanism; separate interfaces from engines.
	\item Rule of Simplicity: Design for simplicity; add complexity only where you must.
	\item Rule of Parsimony: Write a big program only when it is clear by demonstration that nothing else will do.
	\item Rule of Transparency: Design for visibility to make inspection and debugging easier.
	\item Rule of Robustness: Robustness is the child of transparency and simplicity.
	\item Rule of Representation: Fold knowledge into data so program logic can be stupid and robust.
	\item Rule of Least Surprise: In interface design, always do the least surprising thing.
	\item Rule of Silence: When a program has nothing surprising to say, it should say nothing.
	\item Rule of Repair: When you must fail, fail noisily and as soon as possible.
	\item Rule of Economy: Programmer time is expensive; conserve it in preference to machine time.
	\item Rule of Generation: Avoid hand-hacking; write programs to write programs when you can.
	\item Rule of Optimization: Prototype before polishing. Get it working before you optimize it.
	\item Rule of Diversity: Distrust all claims for ``one true way''.
	\item Rule of Extensibility: Design for the future, because it will be here sooner than you think.
\end{itemize}
\end{citequote}
\todo{elaboraaaate *Rule of Simplicity}

By focusing on data extraction we can follow the Rule of Composition more
easily: A layer to map the values we extract to client-side models can still be
implemented on top of it, thereby enabling developers to integrate our tool into
other client libraries than backbone.

\section{Simplifying the project}

Bear in mind that despite the following simplifications we may still use some
of the tools. We intend our tool to perform in an ecosystem of other software,
which can integrate loosely with our tool instead of requiring deep integration
with it.

\subsection{Server-side}
We begin our simplification on the server. Here we communicated with a database
to persist our movies, actors etc. in the MySQL database. The database and the
object relational mapper (ORM) php-activerecord are not at all necessary for our
tool to work. They are interchangeable with any other type of software, that can
persist data on the server. Our concept should work with even ephemeral data.

Our server-side language of choice - PHP -, also belongs to this category.
The server could have been written in any other server-side language.
As a consequence, the template engine (mustache), will of course need to be
able to interface with that language. For the prototype we omitted these
templates and wrote them directly in PHP instead.

Our plan however is to write the server-side templates in mustache.
Locating placeholders in these templates and outputting their location
is the solution we proposed in the beginning of this thesis to the problem
we identified with server side templates.
By extension parsing server side templates pertains to the core of our concept.
Since parsing arbitrary template syntaxes, would go out of the scope of this
thesis we must conclude that mustache belongs to the category of tools that
cannot be removed.

\subsection{Client-side}
The client-side tools we have used in our prototype interact with the data we
"retrieved" (remember: we did not actually retrieve any template information)
from the server. This makes the setup of the client more intricate. We will have
to look carefully at each tool and determine by the nature of its interaction
with that data, whether it is a crucial part of our concept.

Regardless of which tools we remove, we must remember that the information about
our server-side templates must be used somehow. This would suggest that the
client can have more than one structure and set of interconnected parts, which
leverage the additional information.

\begin{itemize}
\item Beginning with the periphery, we can easily see how a framework to ease
the development of CSS is not part of our tool.
\item The JavaScript language is required on the basis that we need some form of
client side programming. We have discussed its alternatives in
\ref{sec:javascript-alts}, depending on the challenges we face in the
implementation, we may choose a language which compiles to JavaScript instead.
\item underscore.js helps us to iterate through arrays and manage other
operations more easily than in pure JavaScript. We can solve the same problems
without it\footnote{
	although it requires more effort.
}. This makes underscore.js a non-crucial part of our tool.
\item We use backbone.js to hold the values we retrieve from the DOM. The
framework enables us to interact with these values. They can however also be
modeled with simple JavaScript objects. Because of that backbone.js can not be
considered a crucial part of our tool.
\end{itemize}

In essence we will not retain any client-side \emph{libraries}.



\subsection{chaplin}

\todo{Figure out if we want to fight the fight and call the view viewmodel}\
\todo{Move this section or delete it}

Chaplin is a new client side framework, which was created in February 2012.
The motivation behind it was to create a framework that allows developers to
follow a set of conventions more easily. Backbone.js has both views and models
(and routes, for controllers), but does not force any specific way of
structuring code. In this respect Backbone.js can be seen more as a tool than a
framework.
Chaplin extends the models and views from Backbone.js and adds more features.
It introduces concepts such as "subviews" - views that aggregate other views.
This allows the developer among other things to better mirror the structure of
the DOM.

The framework also allows the developer to use any template engine he desires.
The engine simply needs to return an object, which jQuery can append to the
wrapping DOM element of the view.

A very useful feature of Chaplin is the automatic memory management.
When creating single page web-applications, the developer has to dispose each
view manually. This challenge is best illustrated with regard to eventhandlers.
Eventhandlers are functions, that are called when an event on a DOM node or
an other object is trigger. Often this function manipulates and accesses
properties stored on a view. To allow for this access, the function stores a
pointer to the view via a closure. Since the function is stored with the
DOM node or object on which it is listening for events, any view the developer
wants to dispose needs to stop listening on those events as well.
Chaplin unbinds these eventhandlers for the developer when the view is disposed,
allowing the browser to free up memory.

The framework has however a major drawback: A view does not add subview elements
to its DOM tree by means of the template function. Instead the developer
must use jQuery to append these elements to an element in the DOM.
Supposing that these subview elements are not attached directly beneath the root
element of the view, the developer will need to traverse parts of the DOM tree
with a jQuery selector to find the correct position to attach a subview element.
This breaks the fundamental principle of dividing the view(-model) from the
template data. A view now has to hold information about the layout of the DOM,
it is hard-coded into the view.

Our tool can solve this problem by supplying the view with those selectors.
The only requirement would be that the developer wrote placeholders into the
templates that identified subviews of the corresponding view.
The view can use the selectors generated for those placeholders
to pinpoint the exact location where a subview element should be attached.

A quicker solution to this problem would be to insert these placeholders and
modify the template engine to understand the concept of subviews.
Unfortunately there is a major drawback to that strategy.
Every subview will be inserted into the DOM when the parent view is rendered,
even though the developer may wish to delay that insertion.
Also, the attachment of subviews that are yet to be created requires a
rerendering of the entire DOM sub-tree.

We intend to modify Chaplin to take advantage of these placeholder selectors
automatically. The developer should not be required to interact with the
selectors.




\todo{What about our initial goal, reading the information from the DOM}
\todo{What about not rerendering ANYTHING? modelbind changes and replace the corresponding nodes}


\end{document}
